{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KdnfImTMTeW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "id": "5KdnfImTMTeW"
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/gdrive/MyDrive/STT_project/data_train.tar.xz '/content/'\n",
        "# !cp /content/gdrive/MyDrive/STT_project/data_dev.tar.xz '/content/'"
      ],
      "metadata": {
        "id": "gsTq4FQiuy1H"
      },
      "id": "gsTq4FQiuy1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -xf /content/data_train.tar.xz\n",
        "# !tar -xf /content/data_dev.tar.xz"
      ],
      "metadata": {
        "id": "7trR7NwevJdu"
      },
      "id": "7trR7NwevJdu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwppWF4KaHrX"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install torchdata\n",
        "!pip install boto3\n",
        "!pip install gradio"
      ],
      "id": "mwppWF4KaHrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA0m1bmkV8XQ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gradio as gr\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from bs4 import BeautifulSoup\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchmetrics.functional import char_error_rate, word_error_rate\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm"
      ],
      "id": "eA0m1bmkV8XQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ub8YHWYagAJ"
      },
      "outputs": [],
      "source": [
        "# instatiate wav2vec2 asr and pretrained object\n",
        "BUNDLE_ASR = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "BUNDLE_PRE = torchaudio.pipelines.WAV2VEC2_BASE"
      ],
      "id": "1ub8YHWYagAJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5iwnNqXnWVV",
        "outputId": "4d951f49-861c-4cec-8eac-72fc0205b264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n",
            "0.13.1+cu116\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(DEVICE)"
      ],
      "id": "G5iwnNqXnWVV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape QA data"
      ],
      "metadata": {
        "id": "OKT-r7S3Jup4"
      },
      "id": "OKT-r7S3Jup4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88894647-06ea-4a39-940b-2c07d93797ec"
      },
      "outputs": [],
      "source": [
        "url_list = [\n",
        "    'https://www.simplilearn.com/tutorials/data-science-tutorial/data-science-interview-questions',\n",
        "    'https://intellipaat.com/blog/interview-question/data-science-interview-questions/',\n",
        "    'https://www.edureka.co/blog/interview-questions/data-science-interview-questions/',\n",
        "    'https://www.interviewbit.com/data-science-interview-questions/',\n",
        "    'https://hackr.io/blog/data-science-interview-questions',\n",
        "    'https://www.springboard.com/blog/data-science/data-science-interview-questions/',\n",
        "    'https://www.interviewkickstart.com/interview-questions/data-scientist-interview-questions',\n",
        "    'https://www.projectpro.io/article/100-data-science-interview-questions-and-answers-for-2021/184',\n",
        "    'https://towardsdatascience.com/over-100-data-scientist-interview-questions-and-answers-c5a66186769a',\n",
        "    'https://brainstation.io/career-guides/data-science-interview-questions',\n",
        "    'https://www.indeed.com/career-advice/interviewing/data-science-interview-questions',\n",
        "    'https://mindmajix.com/data-science-interview-questions',\n",
        "    'https://www.analyticsvidhya.com/blog/2021/04/20-data-science-interview-questions-for-a-beginner/',\n",
        "    'https://www.v7labs.com/blog/data-science-interview-questions-and-answers',\n",
        "    'https://ai.plainenglish.io/data-science-interview-questions-part-2-regression-analysis-fd3406e90b1a',\n",
        "    'https://www.javatpoint.com/data-science-interview-questions',\n",
        "    'https://blog.imocha.io/data-science-interview-questions-and-answers',\n",
        "    'https://firsthand.co/blogs/interviewing/6-common-data-science-interview-questions',\n",
        "    'https://360digitmg.com/top-data-science-interview-questions-answers',\n",
        "    'https://iq.opengenus.org/advanced-interview-questions-on-data-science/',\n",
        "    'https://quanthub.com/data-science-interview-questions/',\n",
        "    'https://skill-lync.com/blogs/top-8-most-frequently-asked-questions-in-data-science-job-interviews',\n",
        "    'https://www.bitdegree.org/tutorials/data-science-interview-questions/',\n",
        "    'https://www.testpreptraining.com/blog/top-70-microsoft-data-science-interview-questions/',\n",
        "    'https://machinelearninggeek.com/data-science-interview-questions-part-6-nlp-text-mining/',\n",
        "    'https://www.hackertrail.com/talent/backend/data-science-interview-questions/',\n",
        "    'https://nitin-panwar.github.io/Top-100-Data-science-interview-questions/'\n",
        "]"
      ],
      "id": "88894647-06ea-4a39-940b-2c07d93797ec"
    },
    {
      "cell_type": "code",
      "source": [
        "question_tokens = ['Q',\n",
        "                  'What',\n",
        "                  'Explain',\n",
        "                  'If ',\n",
        "                  'How',\n",
        "                  'Which',\n",
        "                  'Why',\n",
        "                  'Where',\n",
        "                  'When',\n",
        "                  'Differentiate',\n",
        "                  'In ',\n",
        "                  'You ',\n",
        "                  'For ',\n",
        "                  'Do ',\n",
        "                  'Describe',\n",
        "                  'Difference',\n",
        "                  'Given',\n",
        "                  'Based',\n",
        "                  'We ',\n",
        "                  'Mention',\n",
        "                  'Out ',\n",
        "                  'However',\n",
        "                  'Walkthrough',\n",
        "                  'A ',\n",
        "                  'Can ',\n",
        "                  'Are ',\n",
        "                  'Define',\n",
        "                  'Suppose',\n",
        "                  'During',\n",
        "                  'Give',\n",
        "                  'Is ',\n",
        "                  'Between',\n",
        "                  'State',\n",
        "                  'Please',\n",
        "                  'Supervised',\n",
        "                  'K-Means',\n",
        "                  'Quota',\n",
        "                  'Find',\n",
        "                  'Quickly',\n",
        "                  'Use',\n",
        "                  'Assume',\n",
        "                  'Make',\n",
        "                  'Provide',\n",
        "                  'Consider',\n",
        "                  'An ',\n",
        "                  'To ',\n",
        "                  'List',\n",
        "                  'can',\n",
        "                  'Name',\n",
        "                  'Variance',\n",
        "                  'Calculate',\n",
        "                  'WHAT']"
      ],
      "metadata": {
        "id": "8JVWG9fIJ6g_"
      },
      "id": "8JVWG9fIJ6g_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4897575-8f29-4760-bb3f-d122e246a28d"
      },
      "outputs": [],
      "source": [
        "def scrape_data():\n",
        "    \"\"\"\n",
        "    A function to crawl a list of websites, extract question answer pairs.\n",
        "    Returns dataframe of question and answer columns.\n",
        "    \"\"\"\n",
        "\n",
        "    ds_dict = {}\n",
        "    for count, url in enumerate(tqdm(url_list)):\n",
        "        response = requests.get(url).text\n",
        "        soup = BeautifulSoup(response, \"html.parser\")\n",
        "        for tag in soup.find_all():\n",
        "            if len(tag.text) > 10:\n",
        "                if tag.text.endswith(('?', '? ', '?\\n', '?\\n\\n')) or \\\n",
        "                 tag.text[:1].isdigit() and tag.text[1] == '.' or \\\n",
        "                 tag.text.startswith(tuple(question_tokens)):\n",
        "                    if tag.find_next('p'):\n",
        "                        if tag.text in ds_dict:\n",
        "                            tag_text = f'dup_{count}_{tag.text}'\n",
        "                        else:\n",
        "                            tag_text = tag.text\n",
        "\n",
        "                        if tag.find_next('p').text not in ds_dict.values():\n",
        "                            ds_dict[tag_text] = tag.find_next('p').text\n",
        "\n",
        "    df = pd.DataFrame(list(ds_dict.items()), columns = ['Question','Answer'])\n",
        "    df.to_csv('QA.csv')"
      ],
      "id": "e4897575-8f29-4760-bb3f-d122e246a28d"
    },
    {
      "cell_type": "code",
      "source": [
        "# data_csv = '/content/QA - QA.csv'\n",
        "# df = pd.read_csv(data_csv, names=['Questions', 'Answers'])\n",
        "# df = df.replace('\\n','', regex=True)\n",
        "# df"
      ],
      "metadata": {
        "id": "8OZxPw0ITQWe"
      },
      "id": "8OZxPw0ITQWe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questions = df[\"Questions\"].tolist()\n",
        "# max_len = max([len(question) for question in questions])\n",
        "# min_len = min([len(question) for question in questions])\n",
        "# print(max_len)\n",
        "# print(min_len)"
      ],
      "metadata": {
        "id": "BGHblEbF76vD"
      },
      "id": "BGHblEbF76vD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsnFw1sRVI65"
      },
      "source": [
        "# Preprocess\n"
      ],
      "id": "gsnFw1sRVI65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAo6bu0qkN50"
      },
      "outputs": [],
      "source": [
        "DATASET_TRAIN = \"/content/data_train\""
      ],
      "id": "sAo6bu0qkN50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWWIrXjxaT4B"
      },
      "outputs": [],
      "source": [
        "def get_transcript(transcript_file):\n",
        "    \"\"\"Loads transcript file. Extracts and returns audio transcription.\"\"\"\n",
        "\n",
        "    with open(transcript_file, \"r\", encoding='utf-8') as f:\n",
        "        transcript_line = f.readline().strip()\n",
        "        transcript = re.split('(?<=.){(?=.*)', transcript_line, maxsplit=1, flags=re.IGNORECASE)[0]\n",
        "    return transcript\n",
        "\n",
        "\n",
        "def get_transcript_audio_pair_list(dataset_dir):\n",
        "    \"\"\"\n",
        "    Takes directory of audio and transcript files.\n",
        "    Returns numpy array of audio paths and corresponding transcriptions.\n",
        "    \"\"\"\n",
        "\n",
        "    wavs = set()\n",
        "    transcripts = set()\n",
        "    wav_path = ''\n",
        "    transcript_path = ''\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        if files:\n",
        "            for f in files:\n",
        "                if f.endswith('.wav'):\n",
        "                    wavs.add(f[:-4])\n",
        "                    if not wav_path:\n",
        "                        wav_path = root\n",
        "                if f.endswith('.txt'):\n",
        "                    transcripts.add(f[:-4])\n",
        "                    if not transcript_path:\n",
        "                        transcript_path = root\n",
        "\n",
        "    pair_list = [(f\"{wav_path}/{fname}.wav\", get_transcript(f\"{transcript_path}/{fname}.txt\")) for fname in wavs if fname in transcripts]\n",
        "    return np.asarray(pair_list)"
      ],
      "id": "OWWIrXjxaT4B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2iH6NWogo_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eac7fa9-8090-4987-f371-a83213b97ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['/content/data_train/clips/8000_2_VK_16k_0034.wav'\n",
            "  'Describe Cross-validation Describe Markov chains?']\n",
            " ['/content/data_train/clips/8000_7_SC_16k_mono_0389.wav'\n",
            "  'Explain What is meant by Deep Learning?']\n",
            " ['/content/data_train/clips/8000_1_RP_16k_0111.wav'\n",
            "  'How do you extract features in NLP?']\n",
            " ['/content/data_train/clips/8000_5_SB_16k_0075.wav'\n",
            "  'Explain Unsupervised Clustering approach?']\n",
            " ['/content/data_train/clips/8000_2_VK_16k_0581.wav'\n",
            "  'What is the Sampling Frame in the SRS sampling technique?']]\n"
          ]
        }
      ],
      "source": [
        "data = get_transcript_audio_pair_list(DATASET_TRAIN)\n",
        "print(data[:5])"
      ],
      "id": "F2iH6NWogo_Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "def59ed6-77e0-43e0-94fc-e1df42e416da"
      },
      "source": [
        "## Prepare"
      ],
      "id": "def59ed6-77e0-43e0-94fc-e1df42e416da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztgzTXrehzfJ"
      },
      "outputs": [],
      "source": [
        "# get pretrain model sample rate\n",
        "SAMPLE_RATE = BUNDLE_PRE.sample_rate\n",
        "# load and sort labels\n",
        "LABELS = BUNDLE_ASR.get_labels()\n",
        "LABELS = LABELS + ('[UNK]',)\n",
        "LABELS = sorted(LABELS)\n",
        "# here '-' is arrenged to be at index 0 and corresponds to CTC pad token\n",
        "LABELS = ['-']+[elem for elem in LABELS if elem != '-']"
      ],
      "id": "ztgzTXrehzfJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN98AIaKVHQ8"
      },
      "outputs": [],
      "source": [
        "class InputProcessor:\n",
        "    \"\"\"Class for preprocessing text and audio for training and inference.\"\"\"\n",
        "\n",
        "    def __init__(self, text=False, audio=False, inference=False, labels=[], sample_rate=int()):\n",
        "        \"\"\"Args:\n",
        "            text (bool, optional): Raw text transcriptions. Outputs will be preprocessed text.\n",
        "            audio (bool, optional): Raw audio paths. Outputs will be preprcessed audio.\n",
        "            inference (bool, optional): If True, the output of the class will be\n",
        "            retricted to specific preprocesses.\n",
        "            labels (:obj: list, optional): List of output labels (vocabulary).\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "        \"\"\"\n",
        "\n",
        "        self.text = text\n",
        "        self.audio = audio\n",
        "        self.inference = inference\n",
        "\n",
        "        if self.text and not self.inference:\n",
        "            # create character to index mapping dict. Index used to vectorize transcript.\n",
        "            self.label_to_id_mapping = {char: i for i, char in enumerate(labels)}\n",
        "            # will be used in place of whitespace.\n",
        "            self.delimiter_label = \"|\"\n",
        "        if self.audio:\n",
        "            self.sample_rate = sample_rate\n",
        "\n",
        "    def _clean_transcript(self, transcript):\n",
        "        \"\"\"Clean transcript of all characters except those found in the labels list.\"\"\"\n",
        "\n",
        "        transcript = transcript.replace(\"-\", \" \").replace(\"/\", \" \")\n",
        "        transcript = re.sub(\"[^A-Z' ]\", \"\", transcript.upper())\n",
        "        return transcript\n",
        "\n",
        "    def _tokenize_transcript(self, transcript):\n",
        "        \"\"\"Convert alpha characters to numeric values. Used as output during training.\"\"\"\n",
        "\n",
        "        transcript_token_list = [self.label_to_id_mapping[c] for c in transcript.replace(\" \", self.delimiter_label)]\n",
        "        return transcript_token_list\n",
        "\n",
        "    def _resample_waveform(self, waveform):\n",
        "        \"\"\"Converts sample rate to that of audio trained in pretrained model.\"\"\"\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(waveform)\n",
        "        if sample_rate != self.sample_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sample_rate)\n",
        "        return waveform\n",
        "\n",
        "    def _normalize_audio(self, waveform):\n",
        "        \"\"\"Mean and standard deviation normalization.\n",
        "        Ensures model isn't biased too much to speaker specific features.\"\"\"\n",
        "\n",
        "        waveform_mean = torch.mean(waveform)\n",
        "        waveform_std = torch.std(waveform)\n",
        "        return (waveform - waveform_mean) / (waveform_std + 1e-10)\n",
        "\n",
        "    def __call__(self, input_data):\n",
        "        \"\"\"To apply class preprocessing methods functionally to input text and audio.\"\"\"\n",
        "\n",
        "        if self.text:\n",
        "            transcript = self._clean_transcript(input_data)\n",
        "            if not self.inference:\n",
        "                transcript = transcript + self.delimiter_label\n",
        "                transcript = self._tokenize_transcript(transcript)\n",
        "            return transcript\n",
        "        elif self.audio:\n",
        "            waveform = self._resample_waveform(input_data)\n",
        "            waveform = self._normalize_audio(waveform)\n",
        "            if not self.inference:\n",
        "                waveform_length = waveform.size(1)\n",
        "                return waveform, waveform_length\n",
        "            return waveform\n",
        "\n",
        "\n",
        "class PPDataset(Dataset):\n",
        "    \"\"\"Dataset object to load and process training and eval data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_data, labels, sample_rate):\n",
        "        \"\"\"Args:\n",
        "            input_data (:obj: `list`): Array of audio, transcript arrays.\n",
        "            labels (:obj: list, optional): List of output labels (vocabulary).\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "        \"\"\"\n",
        "\n",
        "        self.tokenizer = InputProcessor(text=True, labels=labels)\n",
        "        self.extractor = InputProcessor(audio=True, sample_rate=sample_rate)\n",
        "        self.input_data = input_data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return len of input_data. \"\"\"\n",
        "\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Preprocesses batch data at index in array of audio, transcript pairs.\n",
        "        Returns set of tensors containing numeric representations of training data.\n",
        "        \"\"\"\n",
        "\n",
        "        audio, text = self.input_data[idx]\n",
        "        waveform, waveform_length = self.extractor(audio)\n",
        "        label = torch.tensor(self.tokenizer(text))\n",
        "        label_length = label.size(0)\n",
        "        return (waveform, waveform_length, label, label_length)"
      ],
      "id": "SN98AIaKVHQ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0LjV6EIt7O-",
        "outputId": "257dfc6c-47a5-4950-d7ba-74dc93045590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-0.0697, -0.0819, -0.0682,  ..., -0.1168, -0.0864, -0.0819]]), 155195, tensor([ 5,  6, 20,  4, 19, 10,  3,  6, 29,  4, 19, 16, 20, 20, 29, 23,  2, 13,\n",
            "        10,  5,  2, 21, 10, 16, 15, 29,  5,  6, 20,  4, 19, 10,  3,  6, 29, 14,\n",
            "         2, 19, 12, 16, 23, 29,  4,  9,  2, 10, 15, 20, 29]), 49)\n",
            "(tensor([[ 0.1269,  0.1214,  0.1338,  ..., -0.6436, -0.6629, -0.6670]]), 76158, tensor([ 6, 25, 17, 13,  2, 10, 15, 29, 24,  9,  2, 21, 29, 10, 20, 29, 14,  6,\n",
            "         2, 15, 21, 29,  3, 26, 29,  5,  6,  6, 17, 29, 13,  6,  2, 19, 15, 10,\n",
            "        15,  8, 29]), 39)\n",
            "(tensor([[ 0.0020,  0.0020,  0.0020,  ...,  0.0004, -0.0012,  0.0004]]), 65598, tensor([ 9, 16, 24, 29,  5, 16, 29, 26, 16, 22, 29,  6, 25, 21, 19,  2,  4, 21,\n",
            "        29,  7,  6,  2, 21, 22, 19,  6, 20, 29, 10, 15, 29, 15, 13, 17, 29]), 35)\n",
            "(tensor([[-0.0257, -0.0264, -0.0312,  ...,  0.0304,  0.0332,  0.0373]]), 65598, tensor([ 6, 25, 17, 13,  2, 10, 15, 29, 22, 15, 20, 22, 17,  6, 19, 23, 10, 20,\n",
            "         6,  5, 29,  4, 13, 22, 20, 21,  6, 19, 10, 15,  8, 29,  2, 17, 17, 19,\n",
            "        16,  2,  4,  9, 29]), 41)\n",
            "(tensor([[0.0058, 0.0116, 0.0443,  ..., 0.0827, 0.0847, 0.0462]]), 155515, tensor([24,  9,  2, 21, 29, 10, 20, 29, 21,  9,  6, 29, 20,  2, 14, 17, 13, 10,\n",
            "        15,  8, 29,  7, 19,  2, 14,  6, 29, 10, 15, 29, 21,  9,  6, 29, 20, 19,\n",
            "        20, 29, 20,  2, 14, 17, 13, 10, 15,  8, 29, 21,  6,  4,  9, 15, 10, 18,\n",
            "        22,  6, 29]), 57)\n"
          ]
        }
      ],
      "source": [
        "ds = PPDataset(data[:5], LABELS, SAMPLE_RATE)\n",
        "for item in ds:\n",
        "    print(item)"
      ],
      "id": "j0LjV6EIt7O-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hubadvzNBsba"
      },
      "outputs": [],
      "source": [
        "class CollateFn:\n",
        "    \"\"\"A custom collate class to process variable length input and ouput batch data.\"\"\"\n",
        "\n",
        "    def pad_data(self, batch, waveforms, labels):\n",
        "        \"\"\"Pads input and output data to length of lengest input/output.\"\"\"\n",
        "\n",
        "        waveform_sizes = [sample[0].shape[1] for sample in batch]\n",
        "        waveform_size = max(waveform_sizes)\n",
        "        waveforms_padded = torch.zeros(len(batch), waveform_size)\n",
        "        for i in range(len(waveforms)):\n",
        "            waveforms_padded[i][0:waveforms[i].shape[1]] = waveforms[i]\n",
        "        labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "        return waveforms_padded, labels_padded\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Applies collate processes functionally to batch data\"\"\"\n",
        "\n",
        "        waveforms = []\n",
        "        wav_lens = []\n",
        "        labels = []\n",
        "        lab_lens = []\n",
        "        for sample in batch:\n",
        "            waveforms.append(sample[0])\n",
        "            wav_lens.append(sample[1])\n",
        "            labels.append(sample[2])\n",
        "            lab_lens.append(sample[3])\n",
        "\n",
        "        wavs_padded, labs_padded = self.pad_data(batch, waveforms, labels)\n",
        "        wav_lens_tensor, lab_lens_tensor = torch.tensor(wav_lens), torch.tensor(lab_lens)\n",
        "        return wavs_padded, wav_lens_tensor.int(), labs_padded.int(), lab_lens_tensor.int()\n",
        "\n",
        "\n",
        "class DataModule(LightningDataModule):\n",
        "    \"\"\"Custom Lightning Data Module class. Prepares and loads training can validation data.\"\"\"\n",
        "\n",
        "    def __init__(self, data, labels, sample_rate, batch_size):\n",
        "        \"\"\"Args:\n",
        "            data (:obj: `list`): Numpy array of audio, transcript arrays.\n",
        "            labels (:obj: list, optional): List of output labels (vocabulary).\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "            batch_size(int): Batch size value.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"Randomized split of train and validation data. Returns sorted splits.\"\"\"\n",
        "\n",
        "        self.train_data, self.val_data = train_test_split(self.data,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True)\n",
        "\n",
        "        # Sort split by output length to ensure similar length datapoints grouped together.\n",
        "        # This is to avoid overpadding of variable length outputs.\n",
        "        self.train_data = sorted(self.train_data, key = lambda x: x[1])\n",
        "        self.val_data = sorted(self.val_data, key = lambda x: x[1])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Loads traning data.\"\"\"\n",
        "\n",
        "        return DataLoader(dataset=PPDataset(self.train_data, self.labels, self.sample_rate),\n",
        "                          batch_size=self.batch_size,\n",
        "                          drop_last=False,\n",
        "                          collate_fn=CollateFn(),\n",
        "                          num_workers=0,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Loads validation data.\"\"\"\n",
        "\n",
        "        return DataLoader(dataset=PPDataset(self.val_data, self.labels, self.sample_rate),\n",
        "                          batch_size=self.batch_size,\n",
        "                          drop_last=False,\n",
        "                          collate_fn=CollateFn(),\n",
        "                          num_workers=0)"
      ],
      "id": "hubadvzNBsba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCEfk_usT4cd",
        "outputId": "220bb310-f8d8-42b5-f9b4-5426af052392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-0.0697, -0.0819, -0.0682,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.1269,  0.1214,  0.1338,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0020,  0.0020,  0.0020,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0058,  0.0116,  0.0443,  ...,  0.0827,  0.0847,  0.0462]]), tensor([155195,  76158,  65598, 155515], dtype=torch.int32), tensor([[ 5,  6, 20,  4, 19, 10,  3,  6, 29,  4, 19, 16, 20, 20, 29, 23,  2, 13,\n",
            "         10,  5,  2, 21, 10, 16, 15, 29,  5,  6, 20,  4, 19, 10,  3,  6, 29, 14,\n",
            "          2, 19, 12, 16, 23, 29,  4,  9,  2, 10, 15, 20, 29, -1, -1, -1, -1, -1,\n",
            "         -1, -1, -1],\n",
            "        [ 6, 25, 17, 13,  2, 10, 15, 29, 24,  9,  2, 21, 29, 10, 20, 29, 14,  6,\n",
            "          2, 15, 21, 29,  3, 26, 29,  5,  6,  6, 17, 29, 13,  6,  2, 19, 15, 10,\n",
            "         15,  8, 29, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "         -1, -1, -1],\n",
            "        [ 9, 16, 24, 29,  5, 16, 29, 26, 16, 22, 29,  6, 25, 21, 19,  2,  4, 21,\n",
            "         29,  7,  6,  2, 21, 22, 19,  6, 20, 29, 10, 15, 29, 15, 13, 17, 29, -1,\n",
            "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "         -1, -1, -1],\n",
            "        [24,  9,  2, 21, 29, 10, 20, 29, 21,  9,  6, 29, 20,  2, 14, 17, 13, 10,\n",
            "         15,  8, 29,  7, 19,  2, 14,  6, 29, 10, 15, 29, 21,  9,  6, 29, 20, 19,\n",
            "         20, 29, 20,  2, 14, 17, 13, 10, 15,  8, 29, 21,  6,  4,  9, 15, 10, 18,\n",
            "         22,  6, 29]], dtype=torch.int32), tensor([49, 39, 35, 57], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "dm = DataModule(data[:5], LABELS, SAMPLE_RATE, batch_size=4)\n",
        "dm.setup()\n",
        "\n",
        "for batch in dm.train_dataloader():\n",
        "    print(batch)"
      ],
      "id": "bCEfk_usT4cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66fe1838-0322-46e8-971c-856e50ce34ea"
      },
      "source": [
        "## Model"
      ],
      "id": "66fe1838-0322-46e8-971c-856e50ce34ea"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transcription(transcription, labels):\n",
        "    \"\"\"Converts vocabulary index to character and returns transcription.\"\"\"\n",
        "\n",
        "    # create dict of vocabulary index, character pairs.\n",
        "    map_dict = {i: char.lower() for i, char in enumerate(labels)}\n",
        "    transcription = \"\".join(map_dict[int(i)] for i in transcription if i != int('-1')).replace(\"|\", \" \").strip()\n",
        "    return transcription\n",
        "\n",
        "\n",
        "def viterbi_decode(emission, labels, blank_idx=0):\n",
        "    \"\"\"\n",
        "    Given a sequence emission over labels, get the highest probability sequence path.\n",
        "    Returns hypothesis transcription.\n",
        "    \"\"\"\n",
        "\n",
        "    hypothesis = emission.argmax(-1).unique_consecutive()\n",
        "    hypothesis = hypothesis[hypothesis != blank_idx]\n",
        "    hypothesis = get_transcription(hypothesis, labels)\n",
        "    return hypothesis\n",
        "\n",
        "\n",
        "def get_error_rates(preds, targets):\n",
        "    \"\"\"Given list of corresponding transcription returns character and word error rates.\"\"\"\n",
        "\n",
        "    wer = word_error_rate(preds=preds, target=targets)\n",
        "    cer = char_error_rate(preds=preds, target=targets)\n",
        "    return cer, wer"
      ],
      "metadata": {
        "id": "0vL2_Roov3wt"
      },
      "id": "0vL2_Roov3wt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-06jQhKG0ks5"
      },
      "outputs": [],
      "source": [
        "class BiStageLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"Custom shceduler class for two stage learning rate scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, warmup_updates, decay_updates):\n",
        "        \"\"\"Args:\n",
        "            optimizer (:obj:): Numpy array of audio, transcript arrays.\n",
        "            warmup_updates (int): number of steps that warmup updates applied.\n",
        "            decay_updates (int): number of steps that decay updates applied.\n",
        "        \"\"\"\n",
        "\n",
        "        self.warmup_updates = warmup_updates\n",
        "        self.decay_updates = decay_updates\n",
        "        # scale multipliers\n",
        "        self.init_lr_scale = 0.01\n",
        "        self.final_lr_scale = 0.05\n",
        "        super().__init__(optimizer, last_epoch=-1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"reduces learing rate by calculated factors. Returns adjust learning rate.\"\"\"\n",
        "\n",
        "        base_lrs_out = []\n",
        "        for base_lr in self.base_lrs:\n",
        "            if self._step_count <= self.warmup_updates:\n",
        "                base_lr = base_lr * (self.init_lr_scale + self._step_count / self.warmup_updates * (1 - self.init_lr_scale))\n",
        "            elif self._step_count <= (self.warmup_updates + self.decay_updates):\n",
        "                base_lr = base_lr * math.exp(math.log(self.final_lr_scale) * (self._step_count - self.warmup_updates) / self.decay_updates)\n",
        "            else:\n",
        "                base_lr = base_lr * self.final_lr_scale\n",
        "            base_lrs_out.append(base_lr)\n",
        "        return base_lrs_out\n",
        "\n",
        "\n",
        "class W2V2CTCModel(LightningModule):\n",
        "    \"\"\"Wav2Vec2 finetune model.\"\"\"\n",
        "\n",
        "    def __init__(self, labels, state_dict={}):\n",
        "        super().__init__()\n",
        "\n",
        "        # instantiate torchaudio model build\n",
        "        self.model = torchaudio.models.wav2vec2_base(\n",
        "            encoder_projection_dropout= 0.1,\n",
        "            encoder_attention_dropout = 0.1,\n",
        "            encoder_ff_interm_dropout = 0.1,\n",
        "            encoder_dropout = 0.1,\n",
        "            encoder_layer_drop = 0.1\n",
        "        )\n",
        "        self.labels = labels\n",
        "        # set output dimension size (length of vocabulary)\n",
        "        ctc_layer_dimension = len(self.labels)\n",
        "        # instantiate linear transform layer. Takes output from encoder.\n",
        "        self.aux = torch.nn.Linear(768, ctc_layer_dimension)\n",
        "        # load pretrained model\n",
        "        self.model.load_state_dict(state_dict, strict=False)\n",
        "        # freeze feature extraction layers\n",
        "        for p in self.model.feature_extractor.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.loss_fn = torch.nn.CTCLoss(zero_infinity=True)\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            list(self.aux.parameters()) + list(self.model.parameters()),\n",
        "            lr=1e-4,\n",
        "            weight_decay=0.005,\n",
        "        )\n",
        "        self.lr_scheduler = BiStageLRScheduler(self.optimizer, 1000, 19000)\n",
        "        # configure lr scheduler to be called at each step\n",
        "        self.lr_scheduler_config = {\n",
        "            \"scheduler\": self.lr_scheduler,\n",
        "            \"interval\": \"step\"\n",
        "        }\n",
        "        # disable automatic lightning optimization (necessary for customizing train step)\n",
        "        self.automatic_optimization = False\n",
        "        # used for mixed precision training in order to reduce gradient underflow\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        # number of step to freezing encoder parameters.\n",
        "        self.freeze_encoder_updates = 1000\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        \"\"\"Forward pass through model, used during inference.\"\"\"\n",
        "\n",
        "        x, lengths = self._get_features(x, lengths)\n",
        "        x = self._encode_features(x, lengths)\n",
        "        x = self.aux(x)\n",
        "        return x\n",
        "\n",
        "    def _get_features(self, wavs, wav_lens):\n",
        "        \"\"\"Extracts feature vectors from raw audio Tensor.\"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            wavs_feats, wav_feats_lens = self.model.feature_extractor(wavs, wav_lens)\n",
        "            return wavs_feats, wav_feats_lens\n",
        "\n",
        "    def _encode_features(self, wavs, wav_lens):\n",
        "        \"\"\"Converts the audio features into the sequence of probability\n",
        "        distribution (in negative log-likelihood) over labels.\"\"\"\n",
        "\n",
        "        x, _ = self.model.encoder._preprocess(wavs, wav_lens)\n",
        "        x = self.model.encoder.transformer(x, attention_mask=None)\n",
        "        return x\n",
        "\n",
        "    def _log_error_rates(self, refs, log_probs):\n",
        "        \"\"\"Estimates and logs char and word error on validation data\"\"\"\n",
        "\n",
        "        targets = [get_transcription(ref, self.labels) for ref in refs]\n",
        "        with torch.inference_mode():\n",
        "            preds = [viterbi_decode(log_prob, self.labels) for log_prob in log_probs]\n",
        "        cer, wer = get_error_rates(preds, targets)\n",
        "        self.log(f\"val_cer\", cer, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(f\"val_wer\", wer, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def _loss_fn(self, batch, batch_idx, step_type=\"\"):\n",
        "        \"\"\"Applies per batch feature extraction and loss estimation.\"\"\"\n",
        "\n",
        "        if batch is not None:\n",
        "            waveforms, waveform_lengths, labels, label_lengths = batch\n",
        "            x, out_len = self._get_features(waveforms, waveform_lengths)\n",
        "            if self.global_step <= self.freeze_encoder_updates:\n",
        "                with torch.no_grad():\n",
        "                    x = self._encode_features(x, out_len)\n",
        "            else:\n",
        "                x = self._encode_features(x, out_len)\n",
        "\n",
        "            aux_logits = self.aux(x)\n",
        "            log_probs = F.log_softmax(aux_logits, dim=-1)\n",
        "            log_probs_trans = log_probs.transpose(0, 1)\n",
        "            loss = self.loss_fn(log_probs_trans, labels, out_len, label_lengths)\n",
        "            self.log(f\"{step_type}_loss\", loss.item()/waveforms.size(0), on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "            if step_type == 'val':\n",
        "                self._log_error_rates(labels, log_probs)\n",
        "            return loss\n",
        "        return batch\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures optimizers.\n",
        "        Return two lists: first is the optimizer's and the second is the scheduler's.\n",
        "        \"\"\"\n",
        "\n",
        "        return [self.optimizer],  [self.lr_scheduler_config]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"Manual optimization training step\"\"\"\n",
        "\n",
        "        opt = self.optimizers()\n",
        "        # resets gradients before parameter updates\n",
        "        opt.zero_grad()\n",
        "        # apply mixed precision training\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                loss = self._loss_fn(batch, batch_idx, \"train\")\n",
        "        except:\n",
        "            with torch.cpu.amp.autocast(enabled=True):\n",
        "                loss = self._loss_fn(batch, batch_idx, \"train\")\n",
        "\n",
        "        # scales loss to prevent vanishing gradients\n",
        "        loss = self.scaler.scale(loss)\n",
        "        self.manual_backward(loss)\n",
        "        # Unscales the gradients of optimizer's assigned params in-place\n",
        "        # Calling before clipping enables you to clip unscaled gradients as usual\n",
        "        self.scaler.unscale_(opt)\n",
        "        # gradient clipping manipulates a set of gradients such that their global norm is <= threshold value\n",
        "        self.clip_gradients(opt, gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\")\n",
        "\n",
        "        # Unscales the gradients of the optimizer's assigned params.\n",
        "        self.scaler.step(opt)\n",
        "        # stepwise scheduler call\n",
        "        sch = self.lr_schedulers()\n",
        "        sch.step()\n",
        "        # Updates the scale for next iteration.\n",
        "        self.scaler.update()\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Estimate validation loss\"\"\"\n",
        "\n",
        "        return self._loss_fn(batch, batch_idx, \"val\")"
      ],
      "id": "-06jQhKG0ks5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f7096e-bbec-44e2-b90f-bd1cdc6999c3"
      },
      "source": [
        "## Train"
      ],
      "id": "a7f7096e-bbec-44e2-b90f-bd1cdc6999c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Topa604L5dhI"
      },
      "outputs": [],
      "source": [
        "# download pretrain model\n",
        "model = BUNDLE_PRE.get_model()\n",
        "# set path for model\n",
        "model_path = 'w2v2_base_pretrained.pt'\n",
        "# save wav2vec2 pretrain model to path\n",
        "torch.save(model, model_path)\n",
        "checkpoint = model_path\n",
        "model_pretrain = torch.load(checkpoint, map_location=torch.device(\"cpu\"))\n",
        "STATE_DICT = copy.deepcopy(model_pretrain.state_dict())\n",
        "\n",
        "\n",
        "def run_train():\n",
        "    \"\"\"Confgure and run training\"\"\"\n",
        "\n",
        "    seed_everything(1337)\n",
        "    checkpoint_dir = \"/content/checkpoints\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        dirpath=checkpoint_dir,\n",
        "        monitor=\"val_wer\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=1,\n",
        "        save_weights_only=True,\n",
        "        verbose=True,\n",
        "    )\n",
        "    early_stopping = EarlyStopping(monitor=\"val_wer\", min_delta=0.00, patience=3, verbose=False, mode=\"min\")\n",
        "    trainer = Trainer(\n",
        "        max_steps=20000,\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        reload_dataloaders_every_n_epochs=1,\n",
        "        accelerator=\"auto\",\n",
        "        val_check_interval=1000,\n",
        "        check_val_every_n_epoch=None\n",
        "    )\n",
        "    data_module = DataModule(data, LABELS, SAMPLE_RATE, batch_size=4)\n",
        "    model = W2V2CTCModel(LABELS, STATE_DICT)\n",
        "    trainer.fit(model, data_module)"
      ],
      "id": "Topa604L5dhI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muwDWm2YURRq"
      },
      "outputs": [],
      "source": [
        "# run_train()"
      ],
      "id": "muwDWm2YURRq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43029efe-c18b-439d-b434-f5657828844d"
      },
      "source": [
        "## Decode"
      ],
      "id": "43029efe-c18b-439d-b434-f5657828844d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe12ffo8KFn4"
      },
      "outputs": [],
      "source": [
        "class Infer:\n",
        "    \"\"\"Class to predict text given speech input\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint, labels, sample_rate):\n",
        "        \"\"\"Args:\n",
        "            checkpoint (str): Trained model checpoint path.\n",
        "            labels (:obj: list, optional): List of output labels (vocabulary).\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.labels = labels\n",
        "        self.model = self.load_checkpoint(checkpoint)\n",
        "\n",
        "        self.tokenizer = InputProcessor(text=True, inference=True)\n",
        "        self.extractor = InputProcessor(audio=True, inference=True, sample_rate=sample_rate)\n",
        "\n",
        "    def load_checkpoint(self, checkpoint):\n",
        "        \"\"\"Load model checkpoint to perform inference.\"\"\"\n",
        "\n",
        "        model = W2V2CTCModel(self.labels)\n",
        "        checkpoint = torch.load(checkpoint, map_location=\"cpu\")\n",
        "        state_dict = checkpoint[\"state_dict\"]\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.eval().to(self.device)\n",
        "        return model\n",
        "\n",
        "    def run_inference(self, waveform):\n",
        "        \"\"\"Run inference on input\"\"\"\n",
        "\n",
        "        waveform = self.extractor(waveform)\n",
        "        with torch.inference_mode():\n",
        "            emission = self.model(waveform.to(self.device))\n",
        "            emission = F.log_softmax(emission, dim=-1)\n",
        "            hypothesis = viterbi_decode(emission, self.labels)\n",
        "        return hypothesis\n",
        "\n",
        "    def run_interactive(self):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        gr.Interface(\n",
        "            fn=self.run_inference,\n",
        "            inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "            outputs=\"text\").launch()\n",
        "\n",
        "    def process_input(self, input_data, evaluate=False):\n",
        "        \"\"\"Preprocess input and either evaluate or generate text\"\"\"\n",
        "\n",
        "        if evaluate:\n",
        "            targets = [self.tokenizer(pair[1]).lower() for pair in input_data]\n",
        "            preds = [self.run_inference(pair[0]) for pair in input_data]\n",
        "            cer, wer = get_error_rates(preds, targets)\n",
        "            for i, target in enumerate(targets[:5]):\n",
        "                print(f\"Reference: {target.lower()}\\nHypothesis: {preds[i]}\\n---------------------------------\")\n",
        "            print(f\"CER: {cer}, WER: {wer}\")\n",
        "        else:\n",
        "            self.run_interactive()"
      ],
      "id": "Qe12ffo8KFn4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ycafSB_EGv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a663d486-0096-4df4-f2c7-48fd7acd00f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference: explain bias variance trade off\n",
            "Hypothesis: explain bias and variance dato\n",
            "---------------------------------\n",
            "Reference: what is understood by the term data science\n",
            "Hypothesis: what is understod by the term data science\n",
            "---------------------------------\n",
            "Reference: explain the differences between supervised and unsupervised learning\n",
            "Hypothesis: explain the differences between supervised and unsupervised learning\n",
            "---------------------------------\n",
            "Reference: how to make a decision tree\n",
            "Hypothesis: how to make a decision tree\n",
            "---------------------------------\n",
            "Reference: can you cite an example where a false positive is more important than a false negative\n",
            "Hypothesis: can you cite an example wher a false positive is more important than a false negative\n",
            "---------------------------------\n",
            "CER: 0.05098039284348488, WER: 0.11627907305955887\n"
          ]
        }
      ],
      "source": [
        "checkpoint = '/content/fintuned_wav2vec2.ckpt'\n",
        "DATASET_DEV = \"/content/data_dev\"\n",
        "input_data = data = get_transcript_audio_pair_list(DATASET_DEV)\n",
        "infer = Infer(checkpoint, LABELS, SAMPLE_RATE)\n",
        "infer.process_input(input_data, evaluate=True)"
      ],
      "id": "0ycafSB_EGv3"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}