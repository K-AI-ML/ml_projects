{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KdnfImTMTeW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "5KdnfImTMTeW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwppWF4KaHrX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning\n",
        "!pip install torchdata\n",
        "!pip install boto3\n",
        "!pip install transformers"
      ],
      "id": "mwppWF4KaHrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA0m1bmkV8XQ",
        "outputId": "b44e9943-4292-4288-f3ee-3b67901700ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "2.0.1+cu118\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import math\n",
        "import os\n",
        "import pytorch_lightning\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import torch\n",
        "import torchaudio\n",
        "import transformers\n",
        "import warnings\n",
        "import multiprocessing as mp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
        "from torchmetrics.functional.classification import multiclass_accuracy, multiclass_f1_score\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    UniSpeechModel,\n",
        "    UniSpeechPreTrainedModel\n",
        ")\n",
        "warnings.filterwarnings('ignore')\n",
        "tqdm.pandas()\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(DEVICE)"
      ],
      "id": "eA0m1bmkV8XQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsnFw1sRVI65"
      },
      "source": [
        "# Preprocess data\n"
      ],
      "id": "gsnFw1sRVI65"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first step, the dataset will be subsampled and balanced to ensure an approximate proportional distribution of classes. Steps include:\n",
        "\n",
        "- Load main data and apply some reformatting\n",
        "- Proportionately subsampling data\n",
        "- Convert audio of subsampled set (mp3 -> wav) and resample frequency to 16Khz\n",
        "- Remove low count classes (sub 100)\n",
        "- Create sepearate class based datasets in dataframe form (e.g. age, accent)\n"
      ],
      "metadata": {
        "id": "BHp62S8kBhzc"
      },
      "id": "BHp62S8kBhzc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9T4r9bjZ1zn"
      },
      "outputs": [],
      "source": [
        "# metadata = pd.read_csv('/content/gdrive/MyDrive/EDA/metadata.csv')\n",
        "# metadata['audio'] = metadata[\"audio\"].apply(lambda x: os.path.split(x)[1])\n",
        "# metadata['utt_len'] = metadata[\"utterance\"].apply(lambda x: len(x))\n",
        "# metadata = metadata[[\"gender\", \"age\", \"accent\", \"utterance\", \"utt_len\", \"audio\", \"duration\"]]\n",
        "# metadata.insert(0, 'id', metadata[\"audio\"].apply(lambda x: x.split('_')[-1][:-4]))\n",
        "# metadata['audio'] = metadata[\"audio\"].apply(lambda x: f\"/content/gdrive/MyDrive/model/audio_processed_original/{x[:-3]}wav\")\n",
        "# metadata"
      ],
      "id": "H9T4r9bjZ1zn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH3DdonIb2ca"
      },
      "outputs": [],
      "source": [
        "def get_counts_perc(df, column):\n",
        "    value_counts = df[column].value_counts()\n",
        "    value_percs = df[column].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
        "    return pd.concat([value_counts, value_percs], axis=1, keys=['count', 'percentage'])"
      ],
      "id": "FH3DdonIb2ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0qCzSDaVhjL"
      },
      "outputs": [],
      "source": [
        "# def balance_features(metadata):\n",
        "\n",
        "#     female_n = 875\n",
        "#     male_n = 375\n",
        "#     age_n = 156\n",
        "#     accent_n = 1250\n",
        "#     feats = {}\n",
        "#     f_list = []\n",
        "\n",
        "#     for index, row in metadata.iterrows():\n",
        "#         if row['gender'] == 'other':\n",
        "#             continue\n",
        "\n",
        "#         if row['accent'] in feats:\n",
        "#             feats[row['accent']]['count'] += 1\n",
        "#         else:\n",
        "#             feats[row['accent']] = {'count': 0}\n",
        "\n",
        "#         if feats[row['accent']]['count'] <= accent_n:\n",
        "#             if row['age'] in feats[row['accent']]:\n",
        "#                 feats[row['accent']][row['age']] += 1\n",
        "#             else:\n",
        "#                 feats[row['accent']][row['age']] = 1\n",
        "\n",
        "#             if feats[row['accent']][row['age']] <= age_n: \n",
        "#                 if row['gender'] in feats[row['accent']]:\n",
        "#                     feats[row['accent']][row['gender']] += 1\n",
        "#                 else:\n",
        "#                     feats[row['accent']][row['gender']] = 1\n",
        "\n",
        "#                 if row['gender'] == 'male' and feats[row['accent']][row['gender']] <= male_n or \\\n",
        "#                  row['gender'] == 'female' and feats[row['accent']][row['gender']] <= female_n :\n",
        "#                     f_list.append({\n",
        "#                         \"gender\": row['gender'],\n",
        "#                         \"age\": row['age'], \n",
        "#                         \"accent\": row['accent'],\n",
        "#                         \"utterance\": row['utterance'],\n",
        "#                         \"utt_len\": row['utt_len'],\n",
        "#                         \"audio\": row['audio'],\n",
        "#                         \"duration\": row['duration'],\n",
        "#                     })\n",
        "#                 else:\n",
        "#                     feats[row['accent']]['count'] -= 1\n",
        "#                     feats[row['accent']][row['age']] -= 1\n",
        "#                     continue\n",
        "\n",
        "#             else:\n",
        "#                 feats[row['accent']]['count'] -= 1\n",
        "#                 continue\n",
        "\n",
        "#     return f_list"
      ],
      "id": "F0qCzSDaVhjL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKLATjtIZ_na"
      },
      "outputs": [],
      "source": [
        "# md_shuffled = metadata.sample(frac=1).reset_index(drop=True)\n",
        "# balanced_list = balance_features(md_shuffled)\n",
        "# meta_balanced_pre = pd.DataFrame(balanced_list)\n",
        "# meta_balanced_pre"
      ],
      "id": "UKLATjtIZ_na"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuhdxl-fb9Ba"
      },
      "outputs": [],
      "source": [
        "# get_counts_perc(meta_balanced_pre, 'gender')"
      ],
      "id": "nuhdxl-fb9Ba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hi-oJFtcKpi"
      },
      "outputs": [],
      "source": [
        "# get_counts_perc(meta_balanced_pre, 'age')"
      ],
      "id": "5Hi-oJFtcKpi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xhaoUUacOFE"
      },
      "outputs": [],
      "source": [
        "# get_counts_perc(meta_balanced_pre, 'accent')"
      ],
      "id": "8xhaoUUacOFE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vdAlWa0mt-B"
      },
      "outputs": [],
      "source": [
        "# filepath = '/content/gdrive/MyDrive/model/meta_balanced_v0.csv'\n",
        "# meta_balanced_pre.to_csv(filepath, index=False)"
      ],
      "id": "4vdAlWa0mt-B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQh25AAGOFtE"
      },
      "outputs": [],
      "source": [
        "def load_dataframe(version=''):\n",
        "    df = pd.read_csv(f'/content/gdrive/MyDrive/model/meta_balanced{version}.csv', low_memory=False)\n",
        "    df.insert(0, 'id', df[\"audio\"].apply(lambda x: x.split('_')[-1][:-4]))\n",
        "    df['audio'] = df[\"audio\"].apply(lambda x: f\"/content/gdrive/MyDrive/model/audio{version}/{x[:-3]}wav\")\n",
        "    return df"
      ],
      "id": "FQh25AAGOFtE"
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataframe('_v3')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "hsQcsYRrFOHo",
        "outputId": "27e14004-2e24-4d79-ea36-ab4754ecbfd5"
      },
      "id": "hsQcsYRrFOHo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id  gender       age  \\\n",
              "0      23809244    male  twenties   \n",
              "1      21091923    male  fourties   \n",
              "2      18750828  female  twenties   \n",
              "3      19057438    male  twenties   \n",
              "4      20969631    male  fourties   \n",
              "...         ...     ...       ...   \n",
              "15145  32443222    male     teens   \n",
              "15146  31324740    male     teens   \n",
              "15147  32639044  female  thirties   \n",
              "15148  30615149    male     teens   \n",
              "15149  31782847    male  thirties   \n",
              "\n",
              "                                                  accent  \\\n",
              "0                                  United States English   \n",
              "1                      German English,Non native speaker   \n",
              "2      India and South Asia (India, Pakistan, Sri Lanka)   \n",
              "3                                       Canadian English   \n",
              "4                      German English,Non native speaker   \n",
              "...                                                  ...   \n",
              "15145  United States English,Southwestern United Stat...   \n",
              "15146                                          Ukrainian   \n",
              "15147                 Northern Irish,Norwegian,yorkshire   \n",
              "15148                 Australian English,England English   \n",
              "15149                                Bangladeshi English   \n",
              "\n",
              "                                               utterance  utt_len  \\\n",
              "0      Married to French Stefani, with whom he later ...       63   \n",
              "1      There was a catapult on either side of the aft...       56   \n",
              "2      It remains hot until the monsoon breaks toward...       67   \n",
              "3      Returning home, Michael does not find Grant an...       77   \n",
              "4           Just to the northeast is the crater Demonax.       44   \n",
              "...                                                  ...      ...   \n",
              "15145  She was the first female in that role at Oneonta.       49   \n",
              "15146  Historically this type of boat was used by Gow...       70   \n",
              "15147  The episode then shows the adventures Susan im...       53   \n",
              "15148  He was assassinated by his political opponents...       57   \n",
              "15149  The cleanly decorated topography room was fill...       77   \n",
              "\n",
              "                                                   audio  duration  \n",
              "0      /content/gdrive/MyDrive/model/audio_v3/common_...     5.664  \n",
              "1      /content/gdrive/MyDrive/model/audio_v3/common_...     6.888  \n",
              "2      /content/gdrive/MyDrive/model/audio_v3/common_...     5.640  \n",
              "3      /content/gdrive/MyDrive/model/audio_v3/common_...     6.840  \n",
              "4      /content/gdrive/MyDrive/model/audio_v3/common_...     6.216  \n",
              "...                                                  ...       ...  \n",
              "15145  /content/gdrive/MyDrive/model/audio_v3/common_...     5.040  \n",
              "15146  /content/gdrive/MyDrive/model/audio_v3/common_...     5.616  \n",
              "15147  /content/gdrive/MyDrive/model/audio_v3/common_...     5.580  \n",
              "15148  /content/gdrive/MyDrive/model/audio_v3/common_...     4.680  \n",
              "15149  /content/gdrive/MyDrive/model/audio_v3/common_...     7.056  \n",
              "\n",
              "[15150 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf10bb65-b3fc-4779-8ad0-33cfd2ca458e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>accent</th>\n",
              "      <th>utterance</th>\n",
              "      <th>utt_len</th>\n",
              "      <th>audio</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23809244</td>\n",
              "      <td>male</td>\n",
              "      <td>twenties</td>\n",
              "      <td>United States English</td>\n",
              "      <td>Married to French Stefani, with whom he later ...</td>\n",
              "      <td>63</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>5.664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21091923</td>\n",
              "      <td>male</td>\n",
              "      <td>fourties</td>\n",
              "      <td>German English,Non native speaker</td>\n",
              "      <td>There was a catapult on either side of the aft...</td>\n",
              "      <td>56</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>6.888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18750828</td>\n",
              "      <td>female</td>\n",
              "      <td>twenties</td>\n",
              "      <td>India and South Asia (India, Pakistan, Sri Lanka)</td>\n",
              "      <td>It remains hot until the monsoon breaks toward...</td>\n",
              "      <td>67</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>5.640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19057438</td>\n",
              "      <td>male</td>\n",
              "      <td>twenties</td>\n",
              "      <td>Canadian English</td>\n",
              "      <td>Returning home, Michael does not find Grant an...</td>\n",
              "      <td>77</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>6.840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20969631</td>\n",
              "      <td>male</td>\n",
              "      <td>fourties</td>\n",
              "      <td>German English,Non native speaker</td>\n",
              "      <td>Just to the northeast is the crater Demonax.</td>\n",
              "      <td>44</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>6.216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15145</th>\n",
              "      <td>32443222</td>\n",
              "      <td>male</td>\n",
              "      <td>teens</td>\n",
              "      <td>United States English,Southwestern United Stat...</td>\n",
              "      <td>She was the first female in that role at Oneonta.</td>\n",
              "      <td>49</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>5.040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15146</th>\n",
              "      <td>31324740</td>\n",
              "      <td>male</td>\n",
              "      <td>teens</td>\n",
              "      <td>Ukrainian</td>\n",
              "      <td>Historically this type of boat was used by Gow...</td>\n",
              "      <td>70</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>5.616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15147</th>\n",
              "      <td>32639044</td>\n",
              "      <td>female</td>\n",
              "      <td>thirties</td>\n",
              "      <td>Northern Irish,Norwegian,yorkshire</td>\n",
              "      <td>The episode then shows the adventures Susan im...</td>\n",
              "      <td>53</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>5.580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15148</th>\n",
              "      <td>30615149</td>\n",
              "      <td>male</td>\n",
              "      <td>teens</td>\n",
              "      <td>Australian English,England English</td>\n",
              "      <td>He was assassinated by his political opponents...</td>\n",
              "      <td>57</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>4.680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15149</th>\n",
              "      <td>31782847</td>\n",
              "      <td>male</td>\n",
              "      <td>thirties</td>\n",
              "      <td>Bangladeshi English</td>\n",
              "      <td>The cleanly decorated topography room was fill...</td>\n",
              "      <td>77</td>\n",
              "      <td>/content/gdrive/MyDrive/model/audio_v3/common_...</td>\n",
              "      <td>7.056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15150 rows Ã— 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf10bb65-b3fc-4779-8ad0-33cfd2ca458e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf10bb65-b3fc-4779-8ad0-33cfd2ca458e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf10bb65-b3fc-4779-8ad0-33cfd2ca458e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_audio(infile):\n",
        "#     outpath = '/content/gdrive/MyDrive/model/audio_v3'\n",
        "#     outfile = f\"{outpath}/{infile[:-4]}.wav\"\n",
        "#     inpath = '/content/audio_original/audio2'\n",
        "#     infile = f\"{inpath}/{infile}\"\n",
        "#     if not os.path.isfile(outfile):\n",
        "#         subprocess.run(['sox', infile, '-r', '16000', outfile])"
      ],
      "metadata": {
        "id": "-Jb-vpMLyKJ6"
      },
      "id": "-Jb-vpMLyKJ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir /content/gdrive/MyDrive/model/audio_v3"
      ],
      "metadata": {
        "id": "9wmrbaH7PvhA"
      },
      "id": "9wmrbaH7PvhA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with mp.Pool(mp.cpu_count()) as pool:\n",
        "#     results = tqdm(\n",
        "#         pool.imap_unordered(convert_audio, mb_df_v3[\"audio\"], chunksize=5),\n",
        "#         total=len(mb_df_v3[\"audio\"]),\n",
        "#     )\n",
        "#     for result in results:\n",
        "#         if result:\n",
        "#             print(result)"
      ],
      "metadata": {
        "id": "hhOxF-9MPa1H"
      },
      "id": "hhOxF-9MPa1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pkRgyuAxVR0"
      },
      "outputs": [],
      "source": [
        "def get_class_data(df):\n",
        "    accent_data = df[df.groupby('accent').accent.transform('count')>100]\n",
        "    age_data = df[df.groupby('age').age.transform('count')>100]\n",
        "    print(len(age_data))\n",
        "    return age_data, accent_data"
      ],
      "id": "7pkRgyuAxVR0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfAyzWo-aXXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072f2808-a12c-42eb-f14d-f4d02fe40cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15150\n"
          ]
        }
      ],
      "source": [
        "AGE_DATA, ACCENT_DATA = get_class_data(mb_df_v3)"
      ],
      "id": "EfAyzWo-aXXI"
    },
    {
      "cell_type": "code",
      "source": [
        "# get_counts_perc(AGE_DATA, 'age')"
      ],
      "metadata": {
        "id": "Jsge4qjACXbQ"
      },
      "id": "Jsge4qjACXbQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(ACCENT_DATA.accent.unique())"
      ],
      "metadata": {
        "id": "km66D4k-Fjj_"
      },
      "id": "km66D4k-Fjj_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_counts_perc(ACCENT_DATA, 'accent')"
      ],
      "metadata": {
        "id": "HcGwEKBUCci_"
      },
      "id": "HcGwEKBUCci_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(AGE_DATA[\"duration\"].sum()/60)/60"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEa2Lc46qXYF",
        "outputId": "34c2b79c-b2a9-434a-acb2-75ef1f3410dd"
      },
      "id": "DEa2Lc46qXYF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.63253399305556"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "def59ed6-77e0-43e0-94fc-e1df42e416da"
      },
      "source": [
        "## Prepare"
      ],
      "id": "def59ed6-77e0-43e0-94fc-e1df42e416da"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data objects to load during training. Objects include:\n",
        "\n",
        "- Processing object to:\n",
        "    - encode labels to from text to integer\n",
        "    - load audio to tensor and optionally resample to correct frequency\n",
        "    - Optionally normalize mean and standard deviation of audio\n",
        "- Dataset object to load and apply processing to elements in a batch.\n",
        "- Custom collate object to:\n",
        "    - Derive attention mask for inputs\n",
        "    - apply batchwise padding to input audio and mask\n",
        "- Datamodule object to:\n",
        "    - prepare train/validation/test data\n",
        "    - sort train and validation by length so batches contain similar length iputs\n",
        "    - Load dataset object\n",
        "    - Load custom collate function\n"
      ],
      "metadata": {
        "id": "wAtz1HOBQyae"
      },
      "id": "wAtz1HOBQyae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztgzTXrehzfJ"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 16000\n",
        "\n",
        "AGE_LABELS = sorted(AGE_DATA.age.unique())\n",
        "AGE_IDX = [i for i, label in enumerate(AGE_LABELS)]\n",
        "\n",
        "ACCENT_LABELS = sorted(ACCENT_DATA.accent.unique())\n",
        "ACCENT_IDX = [i for i, label in enumerate(ACCENT_LABELS)]"
      ],
      "id": "ztgzTXrehzfJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1T4mXQsUX4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8967e3d1-7f86-42c3-9906-5dfb06c9fb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LC_ALL=C.UTF-8\n",
            "env: LANG=C.UTF-8\n",
            "env: TRANSFORMERS_CACHE=/content/cache\n"
          ]
        }
      ],
      "source": [
        "%env LC_ALL=C.UTF-8\n",
        "%env LANG=C.UTF-8\n",
        "%env TRANSFORMERS_CACHE=/content/cache"
      ],
      "id": "l1T4mXQsUX4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFZ0R85WrdEH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class ClassProcessor:\n",
        "    \"\"\"Class for preprocessing text and audio for training and inference.\"\"\"\n",
        "\n",
        "    def __init__(self, labels=[], sample_rate=int()):\n",
        "        \"\"\"Args:\n",
        "            labels (bool, optional): Labes. Outputs will be preprocessed labels.\n",
        "            audio (bool, optional): Raw audio paths. Outputs will be preprcessed audio.\n",
        "            inference (bool, optional): If True, the output of the class will be\n",
        "            retricted to specific preprocesses.\n",
        "            labels (:obj: list, optional): List of output labels (vocabulary).\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "        \"\"\"\n",
        "\n",
        "        self.labels = labels\n",
        "        self.target_sample_rate = sample_rate\n",
        "\n",
        "    def _encode_label(self, label):\n",
        "        \"\"\"Convert labels to numeric values. Used as target during training.\"\"\"\n",
        "        idx = self.labels.index(label) if label in self.labels else -1\n",
        "        return idx\n",
        "\n",
        "    def _resample_waveform(self, waveform):\n",
        "        \"\"\"Converts sample rate to that of audio trained in pretrained model.\"\"\"\n",
        "\n",
        "        speech_array, sample_rate = torchaudio.load(waveform)\n",
        "        if sample_rate != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)\n",
        "            speech_array = resampler(speech_array)\n",
        "        return speech_array\n",
        "\n",
        "    def _normalize_audio(self, waveform):\n",
        "        \"\"\"Mean and standard deviation normalization.\n",
        "        Ensures model isn't biased too much to speaker specific features.\"\"\"\n",
        "\n",
        "        waveform_mean = torch.mean(waveform)\n",
        "        waveform_std = torch.std(waveform)\n",
        "        return (waveform - waveform_mean) / (waveform_std + 1e-10)\n",
        "\n",
        "    def __call__(self, data, column):\n",
        "        \"\"\"To apply class preprocessing methods functionally to input text and audio.\"\"\"\n",
        "\n",
        "        waveform = self._resample_waveform(data['audio'])\n",
        "        # waveform = self._normalize_audio(waveform)\n",
        "        label = self._encode_label(data[column])\n",
        "        return waveform.squeeze().numpy(), label\n",
        "\n",
        "\n",
        "class ClassDataset(Dataset):\n",
        "    \"\"\"Dataset object to load and process training and eval data.\"\"\"\n",
        "\n",
        "    def __init__(self, data, labels, sample_rate, column):\n",
        "        \"\"\"Args:\n",
        "            input_data (:obj: `list`): Array of audio, transcript arrays.\n",
        "            labels (:obj: list, optional): List of output labels (vocabulary).\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "        \"\"\"\n",
        "\n",
        "        self.processor = ClassProcessor(labels, sample_rate)\n",
        "        self.column = column\n",
        "        self.data = data\n",
        "  \n",
        "    def __len__(self):\n",
        "        \"\"\"Return len of input_data. \"\"\"\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Preprocesses batch data at index in array of audio, transcript pairs.\n",
        "        Returns set of tensors containing numeric representations of training data.\n",
        "        \"\"\"\n",
        "        row = self.data.iloc[idx]\n",
        "        try:\n",
        "            waveform, label= self.processor(row, self.column)\n",
        "            return {'input_values': waveform, 'labels': label}\n",
        "        except:\n",
        "            return None\n",
        "            print('no file')"
      ],
      "id": "xFZ0R85WrdEH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-cKajPXtLH5"
      },
      "outputs": [],
      "source": [
        "class CollateFn:\n",
        "    \"\"\"A custom collate class to process variable length input data.\"\"\"\n",
        "\n",
        "    def get_pad(self, waveforms, mask=False):\n",
        "\n",
        "        waveform_size = len(max(waveforms,key=len))\n",
        "        return torch.zeros(len(waveforms), waveform_size)\n",
        "\n",
        "    def pad_data(self, waveforms):\n",
        "        \"\"\"Pads input and masked data to length of longest input.\"\"\"\n",
        "\n",
        "        mask_padded = self.get_pad(waveforms, mask=True)\n",
        "        waveforms_padded = self.get_pad(waveforms)\n",
        "        for i in range(len(waveforms)):\n",
        "            waveforms_padded[i][0:len(waveforms[i])] = torch.tensor(waveforms[i])\n",
        "            mask_padded[i][0:len(waveforms[i])] = torch.ones(len(waveforms[i]))\n",
        "        return waveforms_padded, mask_padded\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Applies collate processes functionally to batch data\"\"\"\n",
        "\n",
        "        input_features = [feature[\"input_values\"] for feature in batch if feature]\n",
        "        label_features = [feature[\"labels\"] for feature in batch if feature]\n",
        "    \n",
        "        try:\n",
        "            waveforms_padded, mask_padded = self.pad_data(input_features)\n",
        "            batch_out = {}\n",
        "            batch_out[\"input_values\"] = waveforms_padded\n",
        "            batch_out[\"attention_mask\"] = mask_padded\n",
        "            batch_out[\"labels\"] = torch.tensor(label_features)\n",
        "            return batch_out\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "\n",
        "class ClassDataModule(LightningDataModule):\n",
        "    \"\"\"Custom Lightning Data Module class. Prepares and loads training, validation, and test data.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        labels,\n",
        "        sample_rate,\n",
        "        out_column,\n",
        "        batch_size\n",
        "    ):\n",
        "        \"\"\"Args:\n",
        "            data (:obj: `dataframe`): Dataframe of raw data.\n",
        "            labels (:obj: list, optional): List of output labels.\n",
        "            sample_rate (int, optional): Integer value sample rate of audio model was trained with.\n",
        "            batch_size(int): Batch size value.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.column = out_column\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"Randomized split of train, validation, and test data. Returns sorted splits.\"\"\"\n",
        "\n",
        "        train_data, dev_data = train_test_split(self.data,\n",
        "                                                test_size=0.2,\n",
        "                                                random_state=123,\n",
        "                                                stratify=self.data[self.column])\n",
        "    \n",
        "        val_data, test_data = train_test_split(dev_data,\n",
        "                                               test_size=0.5,\n",
        "                                               random_state=123,\n",
        "                                               stratify=dev_data[self.column])\n",
        "\n",
        "        train_data = train_data.sort_values(\"duration\")\n",
        "        val_data = val_data.sort_values(\"duration\")\n",
        "\n",
        "        self.train_data = train_data[[self.column, \"audio\"]]\n",
        "        self.val_data = val_data[[self.column, \"audio\"]]\n",
        "        self.test_data = test_data[[self.column, \"audio\"]]\n",
        "            \n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Loads traning data.\"\"\"\n",
        "\n",
        "        return DataLoader(dataset=ClassDataset(self.train_data, self.labels, self.sample_rate, self.column),\n",
        "                          batch_size=self.batch_size,\n",
        "                          drop_last=False,\n",
        "                          collate_fn=CollateFn(),\n",
        "                          num_workers=2,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Loads validation data.\"\"\"\n",
        "\n",
        "        return DataLoader(dataset=ClassDataset(self.val_data, self.labels, self.sample_rate, self.column),\n",
        "                          batch_size=self.batch_size,\n",
        "                          drop_last=False,\n",
        "                          collate_fn=CollateFn(),\n",
        "                          num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"Loads test data.\"\"\"\n",
        "\n",
        "        return DataLoader(dataset=ClassDataset(self.test_data, self.labels, self.sample_rate, self.column),\n",
        "                          batch_size=self.batch_size,\n",
        "                          drop_last=False,\n",
        "                          collate_fn=CollateFn(),\n",
        "                          num_workers=2)"
      ],
      "id": "1-cKajPXtLH5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile Model"
      ],
      "metadata": {
        "id": "i4NX4_mPDK71"
      },
      "id": "i4NX4_mPDK71"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and train a classification model for speech feature classes. Includes:\n",
        "\n",
        "- A model trainer class for loading pretrained model training a classification head.\n",
        "- A classification head for finetuning a self supervised learning pretrained model (Unispeech)\n",
        "- Two stage learning rate scheduler object. \n",
        "- Metrics compute function, which includes accuracy and F1 score."
      ],
      "metadata": {
        "id": "iCmJTPnwcdBM"
      },
      "id": "iCmJTPnwcdBM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "246QhVMVNdXW"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, targets, num_labels):\n",
        "    \"\"\"Takes prediction logits and target labels and computes accuracy and f score.\"\"\"\n",
        "\n",
        "    preds = preds.argmax(-1)\n",
        "    acc = multiclass_accuracy(preds, targets, num_classes=num_labels)\n",
        "    f1 = multiclass_f1_score(preds, targets, num_classes=num_labels, average='macro')\n",
        "    # matrix = confusion_matrix(y_true=targets, y_pred=preds)\n",
        "    # report = classification_report(y_true=targets, y_pred=preds, labels=AGE_IDX, target_names=AGE_LABELS)\n",
        "    return {\"accuracy\": acc, \"f1_score\": f1}"
      ],
      "id": "246QhVMVNdXW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G5xcBGbbVOr"
      },
      "outputs": [],
      "source": [
        "class BiStageLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"Custom shceduler class for two stage learning rate scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, warmup_updates, decay_updates):        \n",
        "        \"\"\"Args:\n",
        "            optimizer (:obj:): Numpy array of audio, transcript arrays.\n",
        "            warmup_updates (int): number of steps that warmup updates applied.\n",
        "            decay_updates (int): number of steps that decay updates applied.\n",
        "        \"\"\"\n",
        "\n",
        "        self.warmup_updates = warmup_updates\n",
        "        self.decay_updates = decay_updates\n",
        "        # scale multipliers\n",
        "        self.init_lr_scale = 0.01\n",
        "        self.final_lr_scale = 0.05\n",
        "        super().__init__(optimizer, last_epoch=-1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"reduces learing rate by calculated factors. Returns adjust learning rate.\"\"\"\n",
        "\n",
        "        base_lrs_out = []\n",
        "        for base_lr in self.base_lrs:\n",
        "            if self._step_count <= self.warmup_updates:\n",
        "                base_lr = base_lr * (self.init_lr_scale + self._step_count / self.warmup_updates * (1 - self.init_lr_scale))\n",
        "            elif self._step_count <= (self.warmup_updates + self.decay_updates):\n",
        "                base_lr = base_lr * math.exp(math.log(self.final_lr_scale) * (self._step_count - self.warmup_updates) / self.decay_updates)\n",
        "            else:\n",
        "                base_lr = base_lr * self.final_lr_scale\n",
        "            base_lrs_out.append(base_lr)\n",
        "        return base_lrs_out"
      ],
      "id": "9G5xcBGbbVOr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCVOWbqLjIeF"
      },
      "outputs": [],
      "source": [
        "class ClassifierModel(UniSpeechPreTrainedModel):\n",
        "    \"\"\"Classification head module. Trains pretrain model towards classification task.\"\"\"\n",
        "\n",
        "    def __init__(self, config, class_weights):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.unispeech = AutoModel.from_config(config)\n",
        "\n",
        "        num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings\n",
        "        self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)\n",
        "\n",
        "        self.feature_extractor = nn.Linear(config.hidden_size, config.hidden_size//2)\n",
        "        self.classifier = nn.Linear(config.hidden_size//2, config.num_labels)\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        \"\"\"\n",
        "        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n",
        "        not be updated during training.\n",
        "        \"\"\"\n",
        " \n",
        "        self.unispeech.feature_extractor._freeze_parameters()\n",
        "\n",
        "    def _get_feat_extract_output_lengths(self, input_lengths):\n",
        "        \"\"\"\n",
        "        Computes the output length of the convolutional layers\n",
        "        \"\"\"\n",
        "\n",
        "        def _conv_out_length(input_length, kernel_size, stride):\n",
        "\n",
        "            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n",
        "\n",
        "        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n",
        "            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n",
        "\n",
        "        return input_lengths\n",
        "\n",
        "    def _get_feature_vector_attention_mask(self, feature_vector_length, attention_mask):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n",
        "        # on inference mode.\n",
        "        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n",
        "        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths).to(torch.long)\n",
        "        batch_size = attention_mask.shape[0]\n",
        "\n",
        "        attention_mask = torch.zeros(\n",
        "            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n",
        "        )\n",
        "        # these two operations makes sure that all values before the output lengths idxs are attended to\n",
        "        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n",
        "        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n",
        "        return attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values,\n",
        "        attention_mask = None,\n",
        "        output_hidden_states = True,\n",
        "        labels = None,\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Generates representations from hidden layers of pretrain model. \n",
        "        Trains meanpooled representations on labels.\n",
        "        \"\"\"\n",
        "\n",
        "        outputs = self.unispeech(\n",
        "            input_values,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=output_hidden_states\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[2]\n",
        "        hidden_states = torch.stack(hidden_states, dim=1)\n",
        "        norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)\n",
        "        hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)\n",
        "\n",
        "        padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n",
        "        hidden_states[~padding_mask] = 0.0\n",
        "        pooling = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n",
        "\n",
        "        hidden_states = self.feature_extractor(pooling)\n",
        "        hidden_states = torch.relu(hidden_states)\n",
        "        logits = self.classifier(hidden_states)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "        return loss, logits"
      ],
      "id": "OCVOWbqLjIeF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vutOp_KSniSN"
      },
      "outputs": [],
      "source": [
        "class ClassTransformer(LightningModule):\n",
        "    \"\"\"Trainer class to load pretrain model and finetune classificaiton head.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name_or_path,\n",
        "        num_labels,\n",
        "        class_weights,\n",
        "        learning_rate = 1e-4,\n",
        "        warmup_steps = 0,\n",
        "        weight_decay = 0.0005\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.num_labels = num_labels\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            num_labels=num_labels,\n",
        "            attention_dropout=0.01,\n",
        "            hidden_dropout=0.05,\n",
        "            layerdrop=0.01,\n",
        "            gradient_checkpointing=True,\n",
        "            use_weighted_layer_sum=True\n",
        "        )\n",
        "        self.model = ClassifierModel.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            config=config,\n",
        "            class_weights=class_weights\n",
        "        )\n",
        "        self.model.freeze_feature_extractor()\n",
        "\n",
        "        self.train_cum_acc = []\n",
        "        self.train_cum_loss = []\n",
        "\n",
        "        # disable automatic lightning optimization (necessary for customizing train step)\n",
        "        self.automatic_optimization = False\n",
        "        # used for mixed precision training in order to reduce gradient underflow\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        \"\"\"Forward pass training with classification head.\"\"\"\n",
        "\n",
        "        return self.model(**inputs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"Manual optimizatoin training step. Includes metric computation.\"\"\"\n",
        "\n",
        "        if batch:\n",
        "            opt = self.optimizers()\n",
        "\n",
        "            # resets gradients before parameter updates\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # apply mixed precision training\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                outputs = self(**batch)    \n",
        "            \n",
        "            loss, logits = outputs\n",
        "\n",
        "            self.train_cum_loss.append(loss.item())\n",
        "            cum_loss = sum(self.train_cum_loss)/len(self.train_cum_loss)\n",
        "\n",
        "            self.log(f\"train_loss\", cum_loss, on_step=True, on_epoch=False, prog_bar=True)\n",
        "\n",
        "            # scales loss to prevent vanishing gradients\n",
        "            loss = self.scaler.scale(loss)\n",
        "            self.manual_backward(loss)\n",
        "\n",
        "            # Unscales the gradients of optimizer's assigned params in-place\n",
        "            # Calling before clipping enables you to clip unscaled gradients as usual\n",
        "            self.scaler.unscale_(opt)\n",
        "\n",
        "            # gradient clipping manipulates a set of gradients such that their global norm is <= threshold value\n",
        "            self.clip_gradients(opt, gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\")\n",
        "\n",
        "            # Unscales the gradients of the optimizer's assigned params.\n",
        "            self.scaler.step(opt)\n",
        "\n",
        "            # stepwise scheduler call\n",
        "            sch = self.lr_schedulers()\n",
        "            sch.step()\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            self.scaler.update()\n",
        "\n",
        "            targets = batch['labels'].to('cpu')\n",
        "            logits = logits.to('cpu')\n",
        "\n",
        "            metrics = compute_metrics(logits, targets, self.num_labels)\n",
        "\n",
        "            self.train_cum_acc.append(metrics[\"accuracy\"])\n",
        "            cum_acc = sum(self.train_cum_acc)/len(self.train_cum_acc)\n",
        "\n",
        "            self.log(f\"train_acc\", cum_acc, on_step=True, on_epoch=False, prog_bar=True)\n",
        "\n",
        "            return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Computes metrics using logits derived at validation step.\"\"\"\n",
        "\n",
        "        if batch:\n",
        "            outputs = self(**batch)\n",
        "            loss, logits = outputs\n",
        "            targets = batch['labels'].to('cpu')\n",
        "            logits = logits.to('cpu')\n",
        "            metrics = compute_metrics(logits, targets, self.num_labels)\n",
        "            \n",
        "            self.log(f\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True) \n",
        "            self.log(f\"val_acc\", metrics[\"accuracy\"], on_step=False, on_epoch=True, prog_bar=True)\n",
        "            return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Prepare optimizer and schedule Bi-stage learning rate scheduler.\"\"\"\n",
        "\n",
        "        scheduler = BiStageLRScheduler(\n",
        "            self.optimizer,\n",
        "            warmup_updates=self.hparams.warmup_steps,\n",
        "            decay_updates=self.trainer.estimated_stepping_batches,\n",
        "        )\n",
        "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
        "        return [self.optimizer], [scheduler]"
      ],
      "id": "vutOp_KSniSN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "981KmVk7DSgW"
      },
      "id": "981KmVk7DSgW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train step function that contains module calls and customized training aspects."
      ],
      "metadata": {
        "id": "NF_ULBz8vnMA"
      },
      "id": "NF_ULBz8vnMA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSU18d02D85M"
      },
      "outputs": [],
      "source": [
        "def run_train_class(data, labels, column, class_weights):\n",
        "    \"\"\"Confgure and run training\"\"\"\n",
        "\n",
        "    seed_everything(123)\n",
        "    checkpoint_dir = \"/content/checkpoints\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        dirpath=checkpoint_dir,\n",
        "        monitor=\"val_acc\",\n",
        "        mode=\"max\",\n",
        "        save_top_k=1,\n",
        "        save_weights_only=True,\n",
        "        verbose=True,\n",
        "    )\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_acc\",\n",
        "        min_delta=0.00,\n",
        "        patience=6,\n",
        "        verbose=False,\n",
        "        mode=\"max\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        max_steps=10000,\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        accelerator=\"auto\",\n",
        "        val_check_interval=500,\n",
        "        log_every_n_steps=500,\n",
        "        check_val_every_n_epoch=None\n",
        "    )\n",
        "    data_module = ClassDataModule(\n",
        "        data=data,\n",
        "        labels=labels,\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        out_column=column,\n",
        "        batch_size=4\n",
        "    )\n",
        "    checkpoint=\"microsoft/unispeech-large-1500h-cv\"\n",
        "    num_labels = len(labels)\n",
        "    model = ClassTransformer(\n",
        "        model_name_or_path=checkpoint,\n",
        "        num_labels=num_labels,\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "    trainer.fit(model, data_module)"
      ],
      "id": "dSU18d02D85M"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_weights(df, column):\n",
        "    \"\"\"Weights classes so that imbalanced data doesn't affect performance.\"\"\"\n",
        "\n",
        "    df_sort = df.sort_values(by=[column])\n",
        "    weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(df_sort[column].tolist()), y=df_sort[column].tolist())\n",
        "    return torch.FloatTensor(weights).cuda()"
      ],
      "metadata": {
        "id": "2h4XBTJNfuY-"
      },
      "id": "2h4XBTJNfuY-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHa6wFwFMCrD"
      },
      "outputs": [],
      "source": [
        "data=ACCENT_DATA\n",
        "labels=ACCENT_LABELS\n",
        "column = 'accent'\n",
        "class_weights = get_class_weights(data, column)\n",
        "run_train_class(data=data, labels=labels, column=column, class_weights=class_weights)"
      ],
      "id": "WHa6wFwFMCrD"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}